{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Lambda\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T00:25:29.894640400Z",
     "start_time": "2024-06-27T00:25:15.994049300Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_input():\n",
    "    X = []\n",
    "    y = []\n",
    "    df = pd.read_csv(\"classification_data/explore_img/explore_data.csv\")\n",
    "    shuffled_df = df.sample(len(df))\n",
    "\n",
    "    print(shuffled_df.head)\n",
    "    s = 0\n",
    "    for entry in shuffled_df.values:\n",
    "        s += 1\n",
    "        # print(s)\n",
    "        entry_img = cv2.imread(f\"classification_data/explore_img/{entry[0]}\")\n",
    "\n",
    "        prepared_y = np.delete(entry,0) * 3\n",
    "        print(f'before: {prepared_y}')\n",
    "        if prepared_y[0] <= 960:\n",
    "            prepared_y[0] =  -(960 - prepared_y[0])\n",
    "        else:\n",
    "            prepared_y[0] = prepared_y[0]-960\n",
    "            \n",
    "        if prepared_y[1] <= 540:\n",
    "            prepared_y[1] = - (540 - prepared_y[1])\n",
    "        else:\n",
    "            prepared_y[1] = prepared_y[1] - 540\n",
    "        print(prepared_y)\n",
    "        \n",
    "        X.append(entry_img)\n",
    "        y.append(prepared_y)\n",
    "        \n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y).astype(np.float32)\n",
    "    return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T00:25:29.895637800Z",
     "start_time": "2024-06-27T00:25:29.854511700Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    image_width, image_height, channels = 640, 360, 3  # Assuming RGB images\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(image_height, image_width, channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "\n",
    "    return model\n",
    "\n",
    "model = compile_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T00:25:30.160927900Z",
     "start_time": "2024-06-27T00:25:29.863722Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T00:25:30.171953600Z",
     "start_time": "2024-06-27T00:25:30.160927900Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of             filename    x    y\n",
      "194  explore_194.jpg  177  242\n",
      "211  explore_211.jpg  254  310\n",
      "290  explore_290.jpg   67  190\n",
      "195  explore_195.jpg  122   68\n",
      "187  explore_187.jpg  521  108\n",
      "..               ...  ...  ...\n",
      "381  explore_381.jpg  126  142\n",
      "120  explore_120.jpg  546  105\n",
      "42    explore_42.jpg  429   60\n",
      "274  explore_274.jpg  186   32\n",
      "149  explore_149.jpg  111   75\n",
      "\n",
      "[403 rows x 3 columns]>\n",
      "before: [531 726]\n",
      "[-429 186]\n",
      "before: [762 930]\n",
      "[-198 390]\n",
      "before: [201 570]\n",
      "[-759 30]\n",
      "before: [366 204]\n",
      "[-594 -336]\n",
      "before: [1563 324]\n",
      "[603 -216]\n",
      "before: [1611 219]\n",
      "[651 -321]\n",
      "before: [1131 936]\n",
      "[171 396]\n",
      "before: [1629 756]\n",
      "[669 216]\n",
      "before: [639 252]\n",
      "[-321 -288]\n",
      "before: [516 837]\n",
      "[-444 297]\n",
      "before: [609 810]\n",
      "[-351 270]\n",
      "before: [645 771]\n",
      "[-315 231]\n",
      "before: [606 705]\n",
      "[-354 165]\n",
      "before: [1701 504]\n",
      "[741 -36]\n",
      "before: [180 555]\n",
      "[-780 15]\n",
      "before: [360 639]\n",
      "[-600 99]\n",
      "before: [1296 168]\n",
      "[336 -372]\n",
      "before: [1185 123]\n",
      "[225 -417]\n",
      "before: [1362 126]\n",
      "[402 -414]\n",
      "before: [480 786]\n",
      "[-480 246]\n",
      "before: [975 1002]\n",
      "[15 462]\n",
      "before: [885 168]\n",
      "[-75 -372]\n",
      "before: [291 837]\n",
      "[-669 297]\n",
      "before: [822 78]\n",
      "[-138 -462]\n",
      "before: [600 897]\n",
      "[-360 357]\n",
      "before: [951 990]\n",
      "[-9 450]\n",
      "before: [564 831]\n",
      "[-396 291]\n",
      "before: [750 81]\n",
      "[-210 -459]\n",
      "before: [324 633]\n",
      "[-636 93]\n",
      "before: [1650 747]\n",
      "[690 207]\n",
      "before: [1365 180]\n",
      "[405 -360]\n",
      "before: [1440 264]\n",
      "[480 -276]\n",
      "before: [1152 99]\n",
      "[192 -441]\n",
      "before: [1062 393]\n",
      "[102 -147]\n",
      "before: [1656 396]\n",
      "[696 -144]\n",
      "before: [516 225]\n",
      "[-444 -315]\n",
      "before: [567 138]\n",
      "[-393 -402]\n",
      "before: [672 123]\n",
      "[-288 -417]\n",
      "before: [237 183]\n",
      "[-723 -357]\n",
      "before: [1506 144]\n",
      "[546 -396]\n",
      "before: [1599 648]\n",
      "[639 108]\n",
      "before: [1080 84]\n",
      "[120 -456]\n",
      "before: [1176 96]\n",
      "[216 -444]\n",
      "before: [1293 120]\n",
      "[333 -420]\n",
      "before: [1137 276]\n",
      "[177 -264]\n",
      "before: [648 855]\n",
      "[-312 315]\n",
      "before: [948 54]\n",
      "[-12 -486]\n",
      "before: [867 987]\n",
      "[-93 447]\n",
      "before: [1593 573]\n",
      "[633 33]\n",
      "before: [1536 909]\n",
      "[576 369]\n",
      "before: [1233 954]\n",
      "[273 414]\n",
      "before: [48 597]\n",
      "[-912 57]\n",
      "before: [1839 765]\n",
      "[879 225]\n",
      "before: [1287 147]\n",
      "[327 -393]\n",
      "before: [738 981]\n",
      "[-222 441]\n",
      "before: [108 153]\n",
      "[-852 -387]\n",
      "before: [1332 105]\n",
      "[372 -435]\n",
      "before: [462 207]\n",
      "[-498 -333]\n",
      "before: [1461 126]\n",
      "[501 -414]\n",
      "before: [1308 93]\n",
      "[348 -447]\n",
      "before: [1197 33]\n",
      "[237 -507]\n",
      "before: [1278 285]\n",
      "[318 -255]\n",
      "before: [1665 660]\n",
      "[705 120]\n",
      "before: [801 150]\n",
      "[-159 -390]\n",
      "before: [1050 564]\n",
      "[90 24]\n",
      "before: [1323 120]\n",
      "[363 -420]\n",
      "before: [1350 168]\n",
      "[390 -372]\n",
      "before: [921 63]\n",
      "[-39 -477]\n",
      "before: [81 594]\n",
      "[-879 54]\n",
      "before: [1329 210]\n",
      "[369 -330]\n",
      "before: [429 741]\n",
      "[-531 201]\n",
      "before: [1470 111]\n",
      "[510 -429]\n",
      "before: [873 252]\n",
      "[-87 -288]\n",
      "before: [1659 597]\n",
      "[699 57]\n",
      "before: [1140 93]\n",
      "[180 -447]\n",
      "before: [549 795]\n",
      "[-411 255]\n",
      "before: [765 309]\n",
      "[-195 -231]\n",
      "before: [1119 132]\n",
      "[159 -408]\n",
      "before: [705 216]\n",
      "[-255 -324]\n",
      "before: [954 90]\n",
      "[-6 -450]\n",
      "before: [1365 303]\n",
      "[405 -237]\n",
      "before: [1485 252]\n",
      "[525 -288]\n",
      "before: [1062 1014]\n",
      "[102 474]\n",
      "before: [1665 348]\n",
      "[705 -192]\n",
      "before: [783 378]\n",
      "[-177 -162]\n",
      "before: [1335 852]\n",
      "[375 312]\n",
      "before: [435 279]\n",
      "[-525 -261]\n",
      "before: [987 753]\n",
      "[27 213]\n",
      "before: [987 306]\n",
      "[27 -234]\n",
      "before: [300 198]\n",
      "[-660 -342]\n",
      "before: [1308 126]\n",
      "[348 -414]\n",
      "before: [489 837]\n",
      "[-471 297]\n",
      "before: [249 768]\n",
      "[-711 228]\n",
      "before: [594 168]\n",
      "[-366 -372]\n",
      "before: [873 150]\n",
      "[-87 -390]\n",
      "before: [1491 264]\n",
      "[531 -276]\n",
      "before: [369 153]\n",
      "[-591 -387]\n",
      "before: [1431 159]\n",
      "[471 -381]\n",
      "before: [843 129]\n",
      "[-117 -411]\n",
      "before: [1776 498]\n",
      "[816 -42]\n",
      "before: [462 213]\n",
      "[-498 -327]\n",
      "before: [1047 111]\n",
      "[87 -429]\n",
      "before: [384 342]\n",
      "[-576 -198]\n",
      "before: [357 264]\n",
      "[-603 -276]\n",
      "before: [885 843]\n",
      "[-75 303]\n",
      "before: [1395 168]\n",
      "[435 -372]\n",
      "before: [1263 141]\n",
      "[303 -399]\n",
      "before: [585 114]\n",
      "[-375 -426]\n",
      "before: [648 63]\n",
      "[-312 -477]\n",
      "before: [546 114]\n",
      "[-414 -426]\n",
      "before: [1275 900]\n",
      "[315 360]\n",
      "before: [636 81]\n",
      "[-324 -459]\n",
      "before: [225 774]\n",
      "[-735 234]\n",
      "before: [1569 138]\n",
      "[609 -402]\n",
      "before: [666 906]\n",
      "[-294 366]\n",
      "before: [1713 585]\n",
      "[753 45]\n",
      "before: [279 297]\n",
      "[-681 -243]\n",
      "before: [429 159]\n",
      "[-531 -381]\n",
      "before: [1041 537]\n",
      "[81 -3]\n",
      "before: [1320 132]\n",
      "[360 -408]\n",
      "before: [579 129]\n",
      "[-381 -411]\n",
      "before: [957 213]\n",
      "[-3 -327]\n",
      "before: [528 87]\n",
      "[-432 -453]\n",
      "before: [1542 243]\n",
      "[582 -297]\n",
      "before: [1020 945]\n",
      "[60 405]\n",
      "before: [339 126]\n",
      "[-621 -414]\n",
      "before: [345 342]\n",
      "[-615 -198]\n",
      "before: [843 927]\n",
      "[-117 387]\n",
      "before: [1014 471]\n",
      "[54 -69]\n",
      "before: [1788 687]\n",
      "[828 147]\n",
      "before: [705 156]\n",
      "[-255 -384]\n",
      "before: [423 864]\n",
      "[-537 324]\n",
      "before: [1017 51]\n",
      "[57 -489]\n",
      "before: [507 315]\n",
      "[-453 -225]\n",
      "before: [705 174]\n",
      "[-255 -366]\n",
      "before: [675 882]\n",
      "[-285 342]\n",
      "before: [858 945]\n",
      "[-102 405]\n",
      "before: [771 987]\n",
      "[-189 447]\n",
      "before: [405 852]\n",
      "[-555 312]\n",
      "before: [339 432]\n",
      "[-621 -108]\n",
      "before: [1359 159]\n",
      "[399 -381]\n",
      "before: [384 321]\n",
      "[-576 -219]\n",
      "before: [1107 336]\n",
      "[147 -204]\n",
      "before: [1098 117]\n",
      "[138 -423]\n",
      "before: [666 933]\n",
      "[-294 393]\n",
      "before: [669 816]\n",
      "[-291 276]\n",
      "before: [1758 555]\n",
      "[798 15]\n",
      "before: [261 411]\n",
      "[-699 -129]\n",
      "before: [1404 234]\n",
      "[444 -306]\n",
      "before: [1473 876]\n",
      "[513 336]\n",
      "before: [969 948]\n",
      "[9 408]\n",
      "before: [300 804]\n",
      "[-660 264]\n",
      "before: [1698 705]\n",
      "[738 165]\n",
      "before: [1536 651]\n",
      "[576 111]\n",
      "before: [1545 894]\n",
      "[585 354]\n",
      "before: [684 912]\n",
      "[-276 372]\n",
      "before: [396 258]\n",
      "[-564 -282]\n",
      "before: [1392 396]\n",
      "[432 -144]\n",
      "before: [945 861]\n",
      "[-15 321]\n",
      "before: [1284 867]\n",
      "[324 327]\n",
      "before: [1350 801]\n",
      "[390 261]\n",
      "before: [777 60]\n",
      "[-183 -480]\n",
      "before: [435 714]\n",
      "[-525 174]\n",
      "before: [306 411]\n",
      "[-654 -129]\n",
      "before: [1338 156]\n",
      "[378 -384]\n",
      "before: [747 111]\n",
      "[-213 -429]\n",
      "before: [1314 183]\n",
      "[354 -357]\n",
      "before: [501 174]\n",
      "[-459 -366]\n",
      "before: [606 885]\n",
      "[-354 345]\n",
      "before: [525 114]\n",
      "[-435 -426]\n",
      "before: [861 1005]\n",
      "[-99 465]\n",
      "before: [966 933]\n",
      "[6 393]\n",
      "before: [1446 261]\n",
      "[486 -279]\n",
      "before: [858 117]\n",
      "[-102 -423]\n",
      "before: [1029 993]\n",
      "[69 453]\n",
      "before: [1260 96]\n",
      "[300 -444]\n",
      "before: [741 924]\n",
      "[-219 384]\n",
      "before: [1587 303]\n",
      "[627 -237]\n",
      "before: [729 96]\n",
      "[-231 -444]\n",
      "before: [1800 402]\n",
      "[840 -138]\n",
      "before: [516 225]\n",
      "[-444 -315]\n",
      "before: [1590 825]\n",
      "[630 285]\n",
      "before: [861 909]\n",
      "[-99 369]\n",
      "before: [372 285]\n",
      "[-588 -255]\n",
      "before: [696 78]\n",
      "[-264 -462]\n",
      "before: [744 978]\n",
      "[-216 438]\n",
      "before: [1077 939]\n",
      "[117 399]\n",
      "before: [1479 129]\n",
      "[519 -411]\n",
      "before: [1509 825]\n",
      "[549 285]\n",
      "before: [1338 126]\n",
      "[378 -414]\n",
      "before: [1323 870]\n",
      "[363 330]\n",
      "before: [1086 903]\n",
      "[126 363]\n",
      "before: [957 114]\n",
      "[-3 -426]\n",
      "before: [1482 828]\n",
      "[522 288]\n",
      "before: [675 243]\n",
      "[-285 -297]\n",
      "before: [1422 843]\n",
      "[462 303]\n",
      "before: [1470 855]\n",
      "[510 315]\n",
      "before: [1311 150]\n",
      "[351 -390]\n",
      "before: [408 147]\n",
      "[-552 -393]\n",
      "before: [600 216]\n",
      "[-360 -324]\n",
      "before: [456 120]\n",
      "[-504 -420]\n",
      "before: [945 1005]\n",
      "[-15 465]\n",
      "before: [384 657]\n",
      "[-576 117]\n",
      "before: [282 690]\n",
      "[-678 150]\n",
      "before: [774 117]\n",
      "[-186 -423]\n",
      "before: [630 789]\n",
      "[-330 249]\n",
      "before: [1212 54]\n",
      "[252 -486]\n",
      "before: [1395 66]\n",
      "[435 -474]\n",
      "before: [1620 711]\n",
      "[660 171]\n",
      "before: [1137 219]\n",
      "[177 -321]\n",
      "before: [1326 153]\n",
      "[366 -387]\n",
      "before: [837 138]\n",
      "[-123 -402]\n",
      "before: [570 843]\n",
      "[-390 303]\n",
      "before: [1467 237]\n",
      "[507 -303]\n",
      "before: [1413 144]\n",
      "[453 -396]\n",
      "before: [744 261]\n",
      "[-216 -279]\n",
      "before: [252 411]\n",
      "[-708 -129]\n",
      "before: [1596 165]\n",
      "[636 -375]\n",
      "before: [1140 48]\n",
      "[180 -492]\n",
      "before: [342 816]\n",
      "[-618 276]\n",
      "before: [627 873]\n",
      "[-333 333]\n",
      "before: [420 402]\n",
      "[-540 -138]\n",
      "before: [918 171]\n",
      "[-42 -369]\n",
      "before: [1353 39]\n",
      "[393 -501]\n",
      "before: [879 951]\n",
      "[-81 411]\n",
      "before: [1230 39]\n",
      "[270 -501]\n",
      "before: [672 813]\n",
      "[-288 273]\n",
      "before: [1428 99]\n",
      "[468 -441]\n",
      "before: [636 852]\n",
      "[-324 312]\n",
      "before: [522 105]\n",
      "[-438 -435]\n",
      "before: [1317 102]\n",
      "[357 -438]\n",
      "before: [1425 132]\n",
      "[465 -408]\n",
      "before: [921 954]\n",
      "[-39 414]\n",
      "before: [1305 84]\n",
      "[345 -456]\n",
      "before: [1230 123]\n",
      "[270 -417]\n",
      "before: [1707 639]\n",
      "[747 99]\n",
      "before: [867 903]\n",
      "[-93 363]\n",
      "before: [669 780]\n",
      "[-291 240]\n",
      "before: [516 213]\n",
      "[-444 -327]\n",
      "before: [1044 408]\n",
      "[84 -132]\n",
      "before: [588 819]\n",
      "[-372 279]\n",
      "before: [459 141]\n",
      "[-501 -399]\n",
      "before: [1197 993]\n",
      "[237 453]\n",
      "before: [1647 669]\n",
      "[687 129]\n",
      "before: [930 909]\n",
      "[-30 369]\n",
      "before: [1167 138]\n",
      "[207 -402]\n",
      "before: [120 195]\n",
      "[-840 -345]\n",
      "before: [1305 87]\n",
      "[345 -453]\n",
      "before: [282 339]\n",
      "[-678 -201]\n",
      "before: [1263 87]\n",
      "[303 -453]\n",
      "before: [1644 825]\n",
      "[684 285]\n",
      "before: [1479 354]\n",
      "[519 -186]\n",
      "before: [288 483]\n",
      "[-672 -57]\n",
      "before: [768 876]\n",
      "[-192 336]\n",
      "before: [1053 90]\n",
      "[93 -450]\n",
      "before: [726 1002]\n",
      "[-234 462]\n",
      "before: [201 639]\n",
      "[-759 99]\n",
      "before: [588 93]\n",
      "[-372 -447]\n",
      "before: [1158 309]\n",
      "[198 -231]\n",
      "before: [1554 834]\n",
      "[594 294]\n",
      "before: [468 819]\n",
      "[-492 279]\n",
      "before: [1341 231]\n",
      "[381 -309]\n",
      "before: [1425 156]\n",
      "[465 -384]\n",
      "before: [582 132]\n",
      "[-378 -408]\n",
      "before: [786 192]\n",
      "[-174 -348]\n",
      "before: [510 108]\n",
      "[-450 -432]\n",
      "before: [909 981]\n",
      "[-51 441]\n",
      "before: [1239 72]\n",
      "[279 -468]\n",
      "before: [1488 564]\n",
      "[528 24]\n",
      "before: [1041 399]\n",
      "[81 -141]\n",
      "before: [1041 120]\n",
      "[81 -420]\n",
      "before: [984 873]\n",
      "[24 333]\n",
      "before: [783 327]\n",
      "[-177 -213]\n",
      "before: [720 231]\n",
      "[-240 -309]\n",
      "before: [207 600]\n",
      "[-753 60]\n",
      "before: [714 999]\n",
      "[-246 459]\n",
      "before: [171 501]\n",
      "[-789 -39]\n",
      "before: [1557 888]\n",
      "[597 348]\n",
      "before: [1836 702]\n",
      "[876 162]\n",
      "before: [1434 171]\n",
      "[474 -369]\n",
      "before: [1449 288]\n",
      "[489 -252]\n",
      "before: [717 126]\n",
      "[-243 -414]\n",
      "before: [546 69]\n",
      "[-414 -471]\n",
      "before: [1467 348]\n",
      "[507 -192]\n",
      "before: [510 321]\n",
      "[-450 -219]\n",
      "before: [996 216]\n",
      "[36 -324]\n",
      "before: [1242 117]\n",
      "[282 -423]\n",
      "before: [681 138]\n",
      "[-279 -402]\n",
      "before: [1086 954]\n",
      "[126 414]\n",
      "before: [1560 378]\n",
      "[600 -162]\n",
      "before: [1314 894]\n",
      "[354 354]\n",
      "before: [1242 978]\n",
      "[282 438]\n",
      "before: [1173 906]\n",
      "[213 366]\n",
      "before: [1428 870]\n",
      "[468 330]\n",
      "before: [1218 255]\n",
      "[258 -285]\n",
      "before: [1374 216]\n",
      "[414 -324]\n",
      "before: [1263 357]\n",
      "[303 -183]\n",
      "before: [1530 750]\n",
      "[570 210]\n",
      "before: [1680 621]\n",
      "[720 81]\n",
      "before: [1053 411]\n",
      "[93 -129]\n",
      "before: [264 756]\n",
      "[-696 216]\n",
      "before: [1527 843]\n",
      "[567 303]\n",
      "before: [1284 75]\n",
      "[324 -465]\n",
      "before: [537 153]\n",
      "[-423 -387]\n",
      "before: [1710 696]\n",
      "[750 156]\n",
      "before: [1716 537]\n",
      "[756 -3]\n",
      "before: [1089 960]\n",
      "[129 420]\n",
      "before: [1008 402]\n",
      "[48 -138]\n",
      "before: [801 1017]\n",
      "[-159 477]\n",
      "before: [846 918]\n",
      "[-114 378]\n",
      "before: [675 171]\n",
      "[-285 -369]\n",
      "before: [834 147]\n",
      "[-126 -393]\n",
      "before: [762 321]\n",
      "[-198 -219]\n",
      "before: [201 375]\n",
      "[-759 -165]\n",
      "before: [1662 759]\n",
      "[702 219]\n",
      "before: [765 93]\n",
      "[-195 -447]\n",
      "before: [174 411]\n",
      "[-786 -129]\n",
      "before: [1800 567]\n",
      "[840 27]\n",
      "before: [1332 204]\n",
      "[372 -336]\n",
      "before: [966 933]\n",
      "[6 393]\n",
      "before: [1476 291]\n",
      "[516 -249]\n",
      "before: [426 156]\n",
      "[-534 -384]\n",
      "before: [288 240]\n",
      "[-672 -300]\n",
      "before: [1638 234]\n",
      "[678 -306]\n",
      "before: [1797 495]\n",
      "[837 -45]\n",
      "before: [495 150]\n",
      "[-465 -390]\n",
      "before: [1350 45]\n",
      "[390 -495]\n",
      "before: [1299 261]\n",
      "[339 -279]\n",
      "before: [1488 672]\n",
      "[528 132]\n",
      "before: [657 207]\n",
      "[-303 -333]\n",
      "before: [1170 576]\n",
      "[210 36]\n",
      "before: [75 471]\n",
      "[-885 -69]\n",
      "before: [1128 156]\n",
      "[168 -384]\n",
      "before: [711 111]\n",
      "[-249 -429]\n",
      "before: [189 678]\n",
      "[-771 138]\n",
      "before: [732 84]\n",
      "[-228 -456]\n",
      "before: [645 216]\n",
      "[-315 -324]\n",
      "before: [696 1026]\n",
      "[-264 486]\n",
      "before: [1728 756]\n",
      "[768 216]\n",
      "before: [1368 174]\n",
      "[408 -366]\n",
      "before: [1164 96]\n",
      "[204 -444]\n",
      "before: [1323 201]\n",
      "[363 -339]\n",
      "before: [1467 171]\n",
      "[507 -369]\n",
      "before: [1593 348]\n",
      "[633 -192]\n",
      "before: [675 942]\n",
      "[-285 402]\n",
      "before: [1176 42]\n",
      "[216 -498]\n",
      "before: [999 111]\n",
      "[39 -429]\n",
      "before: [1251 117]\n",
      "[291 -423]\n",
      "before: [849 129]\n",
      "[-111 -411]\n",
      "before: [1140 147]\n",
      "[180 -393]\n",
      "before: [1599 255]\n",
      "[639 -285]\n",
      "before: [567 810]\n",
      "[-393 270]\n",
      "before: [1701 387]\n",
      "[741 -153]\n",
      "before: [1665 732]\n",
      "[705 192]\n",
      "before: [135 201]\n",
      "[-825 -339]\n",
      "before: [897 996]\n",
      "[-63 456]\n",
      "before: [429 165]\n",
      "[-531 -375]\n",
      "before: [636 858]\n",
      "[-324 318]\n",
      "before: [1770 588]\n",
      "[810 48]\n",
      "before: [270 237]\n",
      "[-690 -303]\n",
      "before: [816 147]\n",
      "[-144 -393]\n",
      "before: [1056 294]\n",
      "[96 -246]\n",
      "before: [1659 483]\n",
      "[699 -57]\n",
      "before: [432 351]\n",
      "[-528 -189]\n",
      "before: [984 69]\n",
      "[24 -471]\n",
      "before: [774 42]\n",
      "[-186 -498]\n",
      "before: [1434 108]\n",
      "[474 -432]\n",
      "before: [1014 924]\n",
      "[54 384]\n",
      "before: [1335 126]\n",
      "[375 -414]\n",
      "before: [1326 843]\n",
      "[366 303]\n",
      "before: [1401 183]\n",
      "[441 -357]\n",
      "before: [1611 378]\n",
      "[651 -162]\n",
      "before: [1446 159]\n",
      "[486 -381]\n",
      "before: [963 987]\n",
      "[3 447]\n",
      "before: [1407 378]\n",
      "[447 -162]\n",
      "before: [939 69]\n",
      "[-21 -471]\n",
      "before: [1011 588]\n",
      "[51 48]\n",
      "before: [639 906]\n",
      "[-321 366]\n",
      "before: [1599 606]\n",
      "[639 66]\n",
      "before: [1056 939]\n",
      "[96 399]\n",
      "before: [606 177]\n",
      "[-354 -363]\n",
      "before: [1245 906]\n",
      "[285 366]\n",
      "before: [1554 258]\n",
      "[594 -282]\n",
      "before: [342 477]\n",
      "[-618 -63]\n",
      "before: [1101 144]\n",
      "[141 -396]\n",
      "before: [1602 243]\n",
      "[642 -297]\n",
      "before: [939 966]\n",
      "[-21 426]\n",
      "before: [807 1050]\n",
      "[-153 510]\n",
      "before: [981 177]\n",
      "[21 -363]\n",
      "before: [372 483]\n",
      "[-588 -57]\n",
      "before: [945 93]\n",
      "[-15 -447]\n",
      "before: [744 372]\n",
      "[-216 -168]\n",
      "before: [924 132]\n",
      "[-36 -408]\n",
      "before: [1218 69]\n",
      "[258 -471]\n",
      "before: [447 768]\n",
      "[-513 228]\n",
      "before: [711 960]\n",
      "[-249 420]\n",
      "before: [708 87]\n",
      "[-252 -453]\n",
      "before: [789 915]\n",
      "[-171 375]\n",
      "before: [378 426]\n",
      "[-582 -114]\n",
      "before: [1638 315]\n",
      "[678 -225]\n",
      "before: [1287 180]\n",
      "[327 -360]\n",
      "before: [558 96]\n",
      "[-402 -444]\n",
      "before: [333 225]\n",
      "[-627 -315]\n",
      "(322, 360, 640, 3) (322, 2)\n"
     ]
    }
   ],
   "source": [
    "X, y = create_input()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T00:49:51.505555800Z",
     "start_time": "2024-06-27T00:49:48.845742400Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 1s/step - loss: 3389673.7500 - val_loss: 312863.4688\n",
      "Epoch 2/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 843ms/step - loss: 243402.7500 - val_loss: 198242.4531\n",
      "Epoch 3/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 852ms/step - loss: 172751.6719 - val_loss: 151730.2344\n",
      "Epoch 4/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 882ms/step - loss: 160992.0938 - val_loss: 145286.2656\n",
      "Epoch 5/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 829ms/step - loss: 156161.5781 - val_loss: 140004.7188\n",
      "Epoch 6/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 812ms/step - loss: 144794.4219 - val_loss: 147473.0781\n",
      "Epoch 7/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 774ms/step - loss: 129132.8828 - val_loss: 189636.1406\n",
      "Epoch 8/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 782ms/step - loss: 150154.9375 - val_loss: 135782.0000\n",
      "Epoch 9/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 795ms/step - loss: 118053.9375 - val_loss: 144915.0938\n",
      "Epoch 10/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 796ms/step - loss: 109525.7500 - val_loss: 127411.0781\n",
      "Epoch 11/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 772ms/step - loss: 80366.9531 - val_loss: 122726.1484\n",
      "Epoch 12/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 790ms/step - loss: 64668.2656 - val_loss: 154152.4375\n",
      "Epoch 13/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 803ms/step - loss: 53920.4453 - val_loss: 139729.7031\n",
      "Epoch 14/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 810ms/step - loss: 60352.8750 - val_loss: 181850.2500\n",
      "Epoch 15/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 811ms/step - loss: 68746.4375 - val_loss: 148453.8281\n",
      "Epoch 16/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 797ms/step - loss: 42104.2656 - val_loss: 140400.5625\n",
      "Epoch 17/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 789ms/step - loss: 31700.2168 - val_loss: 134483.6094\n",
      "Epoch 18/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 798ms/step - loss: 20224.0508 - val_loss: 122858.4453\n",
      "Epoch 19/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 787ms/step - loss: 14725.4688 - val_loss: 128513.2031\n",
      "Epoch 20/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 787ms/step - loss: 14268.9551 - val_loss: 122883.2656\n",
      "Epoch 21/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 820ms/step - loss: 11312.0303 - val_loss: 124529.8203\n",
      "Epoch 22/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 827ms/step - loss: 7577.8076 - val_loss: 117831.4609\n",
      "Epoch 23/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 785ms/step - loss: 8765.8379 - val_loss: 122536.0547\n",
      "Epoch 24/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 777ms/step - loss: 9211.2021 - val_loss: 126917.9297\n",
      "Epoch 25/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 784ms/step - loss: 9593.3232 - val_loss: 119687.8203\n",
      "Epoch 26/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 773ms/step - loss: 8888.9336 - val_loss: 128686.0938\n",
      "Epoch 27/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 780ms/step - loss: 10347.6699 - val_loss: 121244.7344\n",
      "Epoch 28/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 780ms/step - loss: 9961.8633 - val_loss: 118536.9688\n",
      "Epoch 29/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 783ms/step - loss: 6413.2490 - val_loss: 120900.9766\n",
      "Epoch 30/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 801ms/step - loss: 6587.1147 - val_loss: 121097.5156\n",
      "Epoch 31/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 784ms/step - loss: 9180.9072 - val_loss: 134082.5625\n",
      "Epoch 32/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 790ms/step - loss: 8832.1152 - val_loss: 119578.4141\n",
      "Epoch 33/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 815ms/step - loss: 8149.4561 - val_loss: 123591.4219\n",
      "Epoch 34/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 798ms/step - loss: 6380.3198 - val_loss: 123177.1484\n",
      "Epoch 35/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 801ms/step - loss: 8473.8193 - val_loss: 124179.1953\n",
      "Epoch 36/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 788ms/step - loss: 6883.7944 - val_loss: 121726.1328\n",
      "Epoch 37/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 784ms/step - loss: 7095.8628 - val_loss: 121848.7188\n",
      "Epoch 38/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 774ms/step - loss: 4726.7832 - val_loss: 122453.3828\n",
      "Epoch 39/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 782ms/step - loss: 5953.3633 - val_loss: 119999.3906\n",
      "Epoch 40/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 807ms/step - loss: 6516.4438 - val_loss: 128158.1172\n",
      "Epoch 41/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 793ms/step - loss: 5673.5171 - val_loss: 121341.2422\n",
      "Epoch 42/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 779ms/step - loss: 4951.0513 - val_loss: 121897.9453\n",
      "Epoch 43/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 901ms/step - loss: 4253.4810 - val_loss: 121687.7656\n",
      "Epoch 44/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 849ms/step - loss: 4563.1294 - val_loss: 119722.4609\n",
      "Epoch 45/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 890ms/step - loss: 4349.7134 - val_loss: 121331.7109\n",
      "Epoch 46/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 887ms/step - loss: 5805.4692 - val_loss: 120833.8047\n",
      "Epoch 47/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 935ms/step - loss: 4898.6299 - val_loss: 120115.4688\n",
      "Epoch 48/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 1s/step - loss: 6402.9346 - val_loss: 120018.9844\n",
      "Epoch 49/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 985ms/step - loss: 4016.3025 - val_loss: 121196.7031\n",
      "Epoch 50/50\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 933ms/step - loss: 5510.4111 - val_loss: 122144.8516\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x1ec3e5b0980>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T00:31:19.707435500Z",
     "start_time": "2024-06-27T00:25:46.760832Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('mouse_explore_fhd_normalized.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T00:32:10.935312700Z",
     "start_time": "2024-06-27T00:32:08.768044100Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 131ms/step\n",
      "202 196 \n",
      " 108 153\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step\n",
      "1021 255 \n",
      " 378 426\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step\n",
      "822 343 \n",
      " 939 69\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step\n",
      "1328 165 \n",
      " 1425 132\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step\n",
      "1732 669 \n",
      " 1836 702\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 50ms/step\n",
      "811 463 \n",
      " 429 165\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step\n",
      "814 305 \n",
      " 783 378\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step\n",
      "885 509 \n",
      " 1137 219\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step\n",
      "889 596 \n",
      " 1446 261\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step\n",
      "806 528 \n",
      " 360 639\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step\n",
      "1142 239 \n",
      " 1017 51\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step\n",
      "1044 210 \n",
      " 873 150\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step\n",
      "687 339 \n",
      " 531 726\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step\n",
      "1179 852 \n",
      " 1020 945\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step\n",
      "1407 207 \n",
      " 1263 87\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step\n",
      "1179 852 \n",
      " 963 987\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step\n",
      "1393 137 \n",
      " 1332 105\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step\n",
      "919 573 \n",
      " 675 171\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step\n",
      "1012 944 \n",
      " 1062 1014\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step\n",
      "458 275 \n",
      " 288 240\n"
     ]
    }
   ],
   "source": [
    "model_rap = tf.keras.models.load_model('mouse_explore_fhd_normalized2.h5')\n",
    "\n",
    "\n",
    "for n in range(20):\n",
    "    n = n +20\n",
    "    x_real, y_real = int(y_val[n][0]), int(y_val[n][1])\n",
    "    \n",
    "    if x_real <= 960:\n",
    "        x_real =  960 + x_real\n",
    "    else:\n",
    "        x_real = x_real + 960\n",
    "        \n",
    "    if y_real <= 540:\n",
    "        y_real = 540 + y_real \n",
    "    else:\n",
    "        y_real = y_real + 540\n",
    "    \n",
    "    img = np.asarray(X_val[n])\n",
    "\n",
    "    tbp = np.expand_dims(img, axis=0)\n",
    "    img = cv2.resize(img,(1920,1080))\n",
    "    predictions = model_rap.predict(tbp)\n",
    "    x, y = int(predictions[0][0]), int(predictions[0][1])\n",
    "    \n",
    "    if x <= 960:\n",
    "        x =  960 + x\n",
    "    else:\n",
    "        x = x + 960\n",
    "        \n",
    "    if y <= 540:\n",
    "        y = 540 + y \n",
    "    else:\n",
    "        y = y + 540\n",
    "    \n",
    "    print(x, y, \"\\n\",x_real, y_real)\n",
    "    \n",
    "    cv2.circle(img, (x,y), 10, (0,0,255), -1)\n",
    "    cv2.circle(img, (x_real, y_real), 20, (255,255,255), -1)\n",
    "    \n",
    "    cv2.imshow('ss', img)\n",
    "    cv2.waitKey(0) & 0xFF\n",
    "cv2.destroyWindow('ss')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T01:06:18.408794400Z",
     "start_time": "2024-06-27T01:05:54.002414500Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 1s/step - loss: 735715.8750 - val_loss: 131288.5156\n",
      "Epoch 2/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 1s/step - loss: 125163.2031 - val_loss: 122923.0703\n",
      "Epoch 3/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 110525.4766 - val_loss: 100067.1719\n",
      "Epoch 4/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 83076.1016 - val_loss: 80139.3359\n",
      "Epoch 5/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 66418.4766 - val_loss: 83101.9922\n",
      "Epoch 6/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 50298.2266 - val_loss: 69796.7031\n",
      "Epoch 7/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 32745.4199 - val_loss: 67354.6328\n",
      "Epoch 8/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 21479.2988 - val_loss: 81441.3828\n",
      "Epoch 9/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 30637.0371 - val_loss: 72336.4219\n",
      "Epoch 10/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 22397.6641 - val_loss: 73763.8359\n",
      "Epoch 11/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 16824.0508 - val_loss: 69257.7812\n",
      "Epoch 12/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 1s/step - loss: 12744.9414 - val_loss: 61839.5430\n",
      "Epoch 13/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 11298.8857 - val_loss: 63039.6680\n",
      "Epoch 14/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 12085.5342 - val_loss: 70501.0469\n",
      "Epoch 15/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 12147.2363 - val_loss: 63237.1094\n",
      "Epoch 16/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 10150.1504 - val_loss: 69037.8516\n",
      "Epoch 17/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 12442.9668 - val_loss: 62520.5820\n",
      "Epoch 18/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 10251.4062 - val_loss: 66637.3047\n",
      "Epoch 19/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 10554.5020 - val_loss: 63077.5664\n",
      "Epoch 20/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 13736.7080 - val_loss: 67114.7578\n",
      "Epoch 21/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 10284.8896 - val_loss: 66382.6953\n",
      "Epoch 22/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 6963.5010 - val_loss: 64660.0078\n",
      "Epoch 23/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 6636.0347 - val_loss: 66922.5234\n",
      "Epoch 24/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 9434.2529 - val_loss: 64387.5078\n",
      "Epoch 25/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 10875.4707 - val_loss: 66379.8125\n",
      "Epoch 26/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 7913.9561 - val_loss: 63677.1602\n",
      "Epoch 27/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 9138.3125 - val_loss: 66530.8984\n",
      "Epoch 28/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 6537.1655 - val_loss: 64481.9570\n",
      "Epoch 29/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 11396.8721 - val_loss: 68685.7031\n",
      "Epoch 30/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 10867.8486 - val_loss: 67081.9531\n",
      "Epoch 31/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 8548.0205 - val_loss: 66182.0391\n",
      "Epoch 32/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 7706.9219 - val_loss: 64852.4336\n",
      "Epoch 33/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 1s/step - loss: 6716.0391 - val_loss: 65854.8750\n",
      "Epoch 34/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 7366.6001 - val_loss: 66299.6094\n",
      "Epoch 35/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 5482.8428 - val_loss: 66666.9609\n",
      "Epoch 36/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 5300.1260 - val_loss: 65332.7852\n",
      "Epoch 37/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 8167.7183 - val_loss: 65385.1602\n",
      "Epoch 38/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 8553.4424 - val_loss: 65550.9844\n",
      "Epoch 39/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 8595.1768 - val_loss: 65543.4844\n",
      "Epoch 40/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 5699.4097 - val_loss: 64462.3711\n",
      "Epoch 41/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 6988.3491 - val_loss: 66077.5703\n",
      "Epoch 42/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 1s/step - loss: 7032.3677 - val_loss: 66117.3750\n",
      "Epoch 43/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 7626.1890 - val_loss: 66904.7422\n",
      "Epoch 44/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 995ms/step - loss: 7550.5332 - val_loss: 63617.0195\n",
      "Epoch 45/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 995ms/step - loss: 7453.4590 - val_loss: 65011.8906\n",
      "Epoch 46/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 984ms/step - loss: 5465.1377 - val_loss: 66317.6641\n",
      "Epoch 47/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 6420.1279 - val_loss: 67233.6250\n",
      "Epoch 48/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 997ms/step - loss: 8787.9941 - val_loss: 67823.5703\n",
      "Epoch 49/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 9480.9385 - val_loss: 70026.4297\n",
      "Epoch 50/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 9386.1426 - val_loss: 65039.3750\n",
      "Epoch 51/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 6925.6357 - val_loss: 65953.0469\n",
      "Epoch 52/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 989ms/step - loss: 8194.1455 - val_loss: 64728.7773\n",
      "Epoch 53/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 994ms/step - loss: 7642.1763 - val_loss: 64820.0820\n",
      "Epoch 54/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 998ms/step - loss: 8057.7573 - val_loss: 66469.3203\n",
      "Epoch 55/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 1s/step - loss: 7949.4961 - val_loss: 65924.1172\n",
      "Epoch 56/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 6152.3228 - val_loss: 64819.6914\n",
      "Epoch 57/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 982ms/step - loss: 8178.4990 - val_loss: 66051.2500\n",
      "Epoch 58/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 986ms/step - loss: 7306.6348 - val_loss: 69039.0859\n",
      "Epoch 59/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 981ms/step - loss: 6608.1919 - val_loss: 66926.0156\n",
      "Epoch 60/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 997ms/step - loss: 6598.3779 - val_loss: 68771.0625\n",
      "Epoch 61/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 13571.2227 - val_loss: 73333.0000\n",
      "Epoch 62/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 1s/step - loss: 18154.5723 - val_loss: 68523.7031\n",
      "Epoch 63/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - loss: 10342.4229 - val_loss: 63322.3633\n",
      "Epoch 64/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 978ms/step - loss: 9629.6270 - val_loss: 64540.7891\n",
      "Epoch 65/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 965ms/step - loss: 9173.7373 - val_loss: 64104.3281\n",
      "Epoch 66/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 970ms/step - loss: 12480.5762 - val_loss: 63821.1797\n",
      "Epoch 67/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 970ms/step - loss: 9477.4297 - val_loss: 63258.9922\n",
      "Epoch 68/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 992ms/step - loss: 5385.6694 - val_loss: 63738.2578\n",
      "Epoch 69/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 981ms/step - loss: 8221.3145 - val_loss: 66227.6953\n",
      "Epoch 70/70\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 966ms/step - loss: 9139.1094 - val_loss: 67667.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_rap = tf.keras.models.load_model('mouse_explore_fhd_normalized.h5')\n",
    "model_rap.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "\n",
    "model_rap.fit(X_train, y_train, epochs=70, validation_data=(X_val, y_val))\n",
    "model_rap.save('mouse_explore_fhd_normalized2.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T01:04:28.021950800Z",
     "start_time": "2024-06-27T00:50:15.934112300Z"
    }
   },
   "execution_count": 11
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
