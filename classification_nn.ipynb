{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T17:55:29.656503Z",
     "start_time": "2024-07-01T17:55:16.716504Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define input shape based on your image size (here, 640x360)\n",
    "input_shape = (360, 640, 3)\n",
    "\n",
    "# Load MobileNet, excluding the final classification layer\n",
    "mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "\n",
    "x = GlobalAveragePooling2D()(mobilenet.output)\n",
    "\n",
    "predictions = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=mobilenet.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T13:00:21.883700100Z",
     "start_time": "2024-07-01T13:00:21.864208700Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "###default\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "folder_path = 'classification_data/'\n",
    "\n",
    "for folder_name in ['attack_img', 'explore_img']: #, 'loot_img', 'getaway_img']:\n",
    "    folder_full_path = os.path.join(folder_path + folder_name)\n",
    "    \n",
    "    class_label = {\n",
    "      'attack_img': [1,0],\n",
    "      'explore_img': [0,1],\n",
    "      # 'loot_img': [0,0,1,0],\n",
    "      # 'getaway_img': [0,0,0,1]\n",
    "    }[folder_name]\n",
    "    \n",
    "    for filename in os.listdir(folder_full_path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img = cv2.imread(os.path.join(folder_full_path, filename))\n",
    "\n",
    "            X.append(img)\n",
    "            \n",
    "            y.append(class_label)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T17:55:49.704727Z",
     "start_time": "2024-07-01T17:55:35.373926Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T17:55:55.596057Z",
     "start_time": "2024-07-01T17:55:49.704727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## aug\n",
    "folder_path = 'augmented_data/'\n",
    "\n",
    "for folder_name in ['attack_img', 'explore_img']: #, 'loot_img', 'getaway_img']:\n",
    "    folder_full_path = os.path.join(folder_path + folder_name)\n",
    "    \n",
    "    class_label = {\n",
    "      'attack_img': [1,0],\n",
    "      'explore_img': [0,1],\n",
    "      # 'loot_img': [0,0,1,0],\n",
    "      # 'getaway_img': [0,0,0,1]\n",
    "    }[folder_name]\n",
    "    \n",
    "    for filename in os.listdir(folder_full_path):\n",
    "        if filename.endswith(\".jpg\") :\n",
    "            img = cv2.imread(os.path.join(folder_full_path, filename))\n",
    "        \n",
    "            # Append image to X and label to y\n",
    "            X.append(img)\n",
    "            \n",
    "            \n",
    "            y.append(class_label)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T17:55:55.740060Z",
     "start_time": "2024-07-01T17:55:55.596057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n in range(len(X)):\n",
    "    if n % 10 == 0:\n",
    "        X[n] = cv2.cvtColor(X[n], cv2.COLOR_BGR2HSV)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T17:55:57.138112Z",
     "start_time": "2024-07-01T17:55:55.740060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1864, 360, 640, 3) (1864, 2)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T18:02:17.020925Z",
     "start_time": "2024-07-01T17:55:59.430913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_rap = tf.keras.models.load_model('classificator_newgen_augment.h5')\n",
    "model_rap.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_rap.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val))\n",
    "model_rap.save('classificator_newgen_augment2.h5')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m191s\u001B[0m 3s/step - accuracy: 0.8245 - loss: 0.3872 - val_accuracy: 0.8415 - val_loss: 0.3837\n",
      "Epoch 2/2\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m183s\u001B[0m 3s/step - accuracy: 0.8467 - loss: 0.3630 - val_accuracy: 0.8009 - val_loss: 0.3937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T17:54:30.776069Z",
     "start_time": "2024-07-01T17:54:30.175008Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, validation_data\u001B[38;5;241m=\u001B[39m(X_val, y_val))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('classificator_newgen_augment.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T13:24:25.226110700Z",
     "start_time": "2024-07-01T13:24:24.977835100Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "model_rap = tf.keras.models.load_model('classificator_newgen_augment2.h5')\n",
    "\n",
    "\n",
    "for n in range(20):\n",
    "    y_real = y_val[n]\n",
    "    tbp = np.asarray(X_val[n])\n",
    "    cv2.imshow('ss', tbp)\n",
    "    tbp = np.expand_dims(tbp, axis=0)\n",
    "    predictions = model_rap.predict(tbp)\n",
    "    print(predictions, \"\\n\", y_real)\n",
    "\n",
    "    cv2.waitKey(0) & 0xFF\n",
    "cv2.destroyWindow('ss')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:05:55.075284Z",
     "start_time": "2024-07-01T18:04:17.160925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1s/step\n",
      "[[0.73107713 0.1901052 ]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step\n",
      "[[0.5325008  0.15768382]] \n",
      " [0 1]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 110ms/step\n",
      "[[0.04249566 0.6500043 ]] \n",
      " [0 1]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step\n",
      "[[0.56193626 0.2174966 ]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step\n",
      "[[0.3195008  0.41426015]] \n",
      " [0 1]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step\n",
      "[[0.8333505  0.20539805]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 104ms/step\n",
      "[[0.32585075 0.49869403]] \n",
      " [0 1]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step\n",
      "[[0.6378327  0.32990855]] \n",
      " [0 1]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step\n",
      "[[0.6095849 0.5014807]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step\n",
      "[[0.5882802  0.29877174]] \n",
      " [0 1]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step\n",
      "[[0.848505   0.10922904]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step\n",
      "[[0.49784544 0.38235372]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step\n",
      "[[0.3201364  0.51327956]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 120ms/step\n",
      "[[0.87436616 0.06132353]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step\n",
      "[[0.78483725 0.11527899]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 106ms/step\n",
      "[[0.81132615 0.0695824 ]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step\n",
      "[[0.68450177 0.12978286]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step\n",
      "[[0.9047812  0.06247845]] \n",
      " [1 0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step\n",
      "[[0.41519058 0.3576645 ]] \n",
      " [0 1]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step\n",
      "[[0.38954076 0.38664094]] \n",
      " [0 1]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "augmented data fragment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m134s\u001B[0m 2s/step - accuracy: 0.6969 - loss: 0.6479 - val_accuracy: 0.7436 - val_loss: 0.5233\n",
      "Epoch 2/5\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m116s\u001B[0m 2s/step - accuracy: 0.7596 - loss: 0.4912 - val_accuracy: 0.7543 - val_loss: 0.4926\n",
      "Epoch 3/5\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m115s\u001B[0m 2s/step - accuracy: 0.7682 - loss: 0.4717 - val_accuracy: 0.7436 - val_loss: 0.4929\n",
      "Epoch 4/5\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m114s\u001B[0m 2s/step - accuracy: 0.7594 - loss: 0.4907 - val_accuracy: 0.7286 - val_loss: 0.4974\n",
      "Epoch 5/5\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m121s\u001B[0m 2s/step - accuracy: 0.7698 - loss: 0.4797 - val_accuracy: 0.7350 - val_loss: 0.4821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_rap = tf.keras.models.load_model('classificator_6_augmented.h5')\n",
    "model_rap.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "model_rap.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "model_rap.save('classificator_6_augmented_hsv.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T12:44:27.541684Z",
     "start_time": "2024-07-01T12:34:25.592886300Z"
    }
   },
   "execution_count": 5
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
